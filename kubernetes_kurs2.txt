
######################################################
Pełny K8s
######################################################


Przygotuj nazwy hostów i /etc/hosts (na każdej maszynie)
Ustaw hostname:
Na control-plane:
sudo hostnamectl set-hostname cp1
Na workerach:
sudo hostnamectl set-hostname w1# na drugiej
sudo hostnamectl set-hostname w2
Dodaj wpisy do /etc/hosts (na wszystkich 3):
Podmień IP na swoje:
sudo nano /etc/hosts
Dodaj:
10.4.0.10 cp1
10.4.0.100 w1
10.4.0.200 w2
1) Wymagania systemowe: swap off + moduły + sysctl (na wszystkich 3)

Wyłącz swap:
sudo swapoff -a
Wyłącz swap na stałe (usuń/zakomentuj linię swap w /etc/fstab):
sudo sed -i.bak '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

Moduły kernela:
sudo tee /etc/modules-load.d/k8s.conf >/dev/null <<'EOF'
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter

Sysctl dla K8s:
sudo tee /etc/sysctl.d/k8s.conf >/dev/null <<'EOF'
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo sysctl --system
Sprawdź:
lsmod | egrep 'overlay|br_netfilter'
sysctl net.ipv4.ip_forward
2) Zainstaluj containerd (na wszystkich 3)

W sytuacji kiedy był już containerd

apt remove containerd.io
mv /etc/apt/sources.list.d/docker.list /tmp/docker.list
apt update
apt -y install containerd

cat
sudo apt update
sudo apt install -y containerd
Wygeneruj konfigurację i ustaw SystemdCgroup = true:
sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml >/dev/null
Edytuj:
sudo nano -c /etc/containerd/config.toml
Znajdź sekcję:
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
i ustaw:
SystemdCgroup = true
Restart:
sudo systemctl restart containerd
sudo systemctl enable containerd
sudo systemctl status containerd --no-pager


Kiedy i po co to ustawiać:
    1. Lepsza integracja z systemd (GŁÓWNY POWÓD)
Pozwala na lepsze zarządzanie cgroups przez systemd
Kontenery są widziane jako natywne jednostki systemd
Umożliwia użycie systemd-cgls, systemd-cgtop do monitorowania2. 
Hierarchia cgroup v2
Wymagane dla poprawnego działania z cgroup v2
Nowe dystrybucje (Ubuntu 22.04+, RHEL 9+, Fedora 31+) domyślnie używają cgroup v23. Resource management
Lepsze zarządzanie zasobami systemowymi
Poprawne działanie limitów pamięci/CPU
Wsparcie dla memory QoS



3) Zainstaluj kubeadm/kubelet/kubectl (na wszystkich 3)
Na Ubuntu 24.04 użyj repo pkgs.k8s.io (stabilne). Poniżej przykład dla v1.30 — jeśli chcesz inną wersję, zmień v1.30 na np. v1.35.
sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg
Dodaj klucz i repo:
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.35/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg 
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.35/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list | sudo tee /etc/apt/sources.list.d/kubernetes.list
Instalacja:
sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
Zablokuj wersje (żeby apt sam nie podniósł bez kontroli):
sudo apt-mark hold kubelet kubeadm kubectl

sudo systemctl enable --now kubelet

4) Inicjalizacja control-plane (tylko na cp1)
nano -c /etc/containerd/config.toml
Upewnij się, że control-plane ma poprawny IP (sprawdź):
ip a
kubeadm init
Podmień --apiserver-advertise-address na IP cp1:
sudo kubeadm init \  --apiserver-advertise-address=10.4.0.10 \  --pod-network-cidr=192.168.0.0/16
Na końcu dostaniesz:
komendy do ustawienia kubeconfig
kubeadm join ... dla workerów
Skonfiguruj kubectl dla użytkownika:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown "$(id -u)":"$(id -g)" $HOME/.kube/config
Sprawdź:
kubectl get nodes
Na tym etapie node będzie NotReady (bo brak CNI).

5) Zainstaluj CNI (Calico) (na cp1)
Najprościej manifestem:
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.1/manifests/calico.yaml
Poczekaj aż systemowe pody wstaną:
kubectl get pods -A -w
Gdy Calico będzie Running, node cp1 powinien przejść na Ready:
kubectl get nodes
6) Dołącz workery do klastra (na w1 i w2)
Na workerach wklej dokładnie kubeadm join ... które wypisał kubeadm init.
Wygląda to mniej więcej tak:
sudo kubeadm join 10.0.0.10:6443 --token <TOKEN> \  --discovery-token-ca-cert-hash sha256:<HASH>
Po dołączeniu sprawdź na cp1:
kubectl get nodes
Powinieneś zobaczyć cp1, w1, w2 jako Ready.
7) Test klastra: deployment + Service
Uruchom testowy nginx:
kubectl create deployment nginx --image=nginx
kubectl expose deployment nginx --port 80 --type NodePort
kubectl get pods -o wide
kubectl get svc
Zobacz NodePort (np. 3xxxx) i wejdź z przeglądarki/curla:
curl http://IP_WORKERA:NODEPORT

8) (Opcjonalnie) Pozwól uruchamiać pody na control-plane
Domyślnie control-plane jest “taintowany” (nie przyjmuje zwykłych podów). Jeśli to mały lab i chcesz:
kubectl taint nodes cp1 node-role.kubernetes.io/control-plane-

9) Najczęstsze problemy (szybkie diagnozy)Node NotReady
brak CNI albo CNI nie wstało:
kubectl get pods -A
kubelet nie działa / crashuje
sudo systemctl status kubelet --no-pagersudo journalctl -u kubelet -n 200 --no-pager


2026-02-01 09:37:16.800 [INFO][1] main.go 130: Failed to initialize datastore error=Get "https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default": dial tcp 10.96.0.1:443: i/o timeout
2026-02-01 09:37:46.820 [ERROR][1] client.go 287: Error getting cluster information config ClusterInformation="default" error=Get "https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default": context deadline exceeded
2026-02-01 09:37:46.820 [INFO][1] main.go 130: Failed to initialize datastore error=Get "https://10.96.0.1:443/apis/crd.projectcalico.org/v1/clusterinformations/default": context deadline exceeded
2026-02-01 09:37:46.820 [FATAL][1] main.go 143: Failed to initialize Calico datastore




11) Kubernetes Dashboard

Instalacja dashboardu
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
Sprawdź:
kubectl get pods -n kubernetes-dashboard

12) Utworzenie użytkownika admin (RBAC)
Dashboard nie działa bez tokena – robimy konto admina.
ServiceAccount
kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
ClusterRoleBinding (pełne prawa – LAB!)
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin

13) Pobranie tokena logowaniaKubernetes ≥ 1.24 (token dynamiczny)
kubectl -n kubernetes-dashboard create token dashboard-admin
Skopiuj token — będzie potrzebny do logowania.

14) Dostęp do DashboarduOpcja A: Port-forward (NAJPROSTSZA, polecana)
kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard 8443:443&
Otwórz w przeglądarce:
https://localhost:8443
Wybierz Token i wklej token admina.
Opcja B: NodePort (LAB / nieprodukcyjne)
Edytuj service:
kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard
Zmień:
type: ClusterIP
na:
type: NodePort
Sprawdź port:
kubectl -n kubernetes-dashboard get svc
Wejście:
https://IP_WORKERA:NODEPORT
Nie zalecane produkcyjnie
15) Szybka weryfikacja całości
kubectl get nodeskubectl top nodeskubectl top pods -Akubectl get pods -n kubernetes-dashboarda
W Dashboardzie sprawdź:
Nodes
Workloads
Metrics (CPU / Memory)16) (Opcjonalnie) Automatyczny dostęp z control-plane
Alias:
echo "alias kdash='kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard 8443:443'" >> ~/.bashrcsource ~/.bashrc
17) Ważne uwagi bezpieczeństwa
cluster-admin tylko do labów
W produkcji:
osobne Role/RoleBinding
dostęp przez Ingress + auth
certyfikaty + OIDC (Keycloak / Dex)



Metrics Server (wymagany m.in. dla kubectl top, HPA)Instalacja (na control-plane)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
Poprawka pod kubeadm + containerd 

W klastrach kubeadm Metrics Server musi ignorować self-signed certyfikaty kubeleta, inaczej nie wstanie.
Edytuj deployment:
kubectl edit deployment metrics-server -n kube-system
Znajdź sekcję containers -> args i dodaj:
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname

spec:
nodeName: 00ubuntu # to jest kluczowy wpis - Dzięki Daniel
containers:
- args:
- --cert-dir=/tmp
- --secure-port=10250
- --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
- --kubelet-use-node-status-port
- --metric-resolution=15s
command:
- /metrics-server
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP

Zapisz i wyjdź.
Sprawdzenie statusu
kubectl get pods -n kube-system | grep metrics
Po ~30–60 sekundach:
kubectl top nodes
kubectl top pods -A
Jeśli działa → Metrics Server OK


11) Kubernetes Dashboard

Instalacja dashboardu
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
Sprawdź:
kubectl get pods -n kubernetes-dashboard

12) Utworzenie użytkownika admin (RBAC)
Dashboard nie działa bez tokena – robimy konto admina.
ServiceAccount
kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
ClusterRoleBinding (pełne prawa – LAB!)
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin

13) Pobranie tokena logowaniaKubernetes ≥ 1.24 (token dynamiczny)
kubectl -n kubernetes-dashboard create token dashboard-admin
Skopiuj token — będzie potrzebny do logowania.

14) Dostęp do DashboarduOpcja A: Port-forward (NAJPROSTSZA, polecana)
kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard 8443:443&
Otwórz w przeglądarce:
https://localhost:8443
Wybierz Token i wklej token admina.
Opcja B: NodePort (LAB / nieprodukcyjne)
Edytuj service:
kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard
Zmień:
type: ClusterIP
na:
type: NodePort
Sprawdź port:
kubectl -n kubernetes-dashboard get svc
Wejście:
https://IP_WORKERA:NODEPORT
Nie zalecane produkcyjnie
15) Szybka weryfikacja całości
kubectl get nodeskubectl top nodeskubectl top pods -Akubectl get pods -n kubernetes-dashboarda
W Dashboardzie sprawdź:
Nodes
Workloads
Metrics (CPU / Memory)16) (Opcjonalnie) Automatyczny dostęp z control-plane
Alias:
echo "alias kdash='kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard 8443:443'" >> ~/.bashrcsource ~/.bashrc
17) Ważne uwagi bezpieczeństwa
cluster-admin tylko do labów
W produkcji:
osobne Role/RoleBinding
dostęp przez Ingress + auth
certyfikaty + OIDC (Keycloak / Dex)




MI poszło przy: 
        spec:
      nodeName: 00ubuntu
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=10250
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        image: registry.k8s.io/metrics-server/metrics-server:v0.8.1
        imagePullPolicy: IfNotPresent
        

export KUBECONFIG=/etc/kubernetes/admin.conf

W Azure NSG / firewall / security group:
ALLOW
Protocol: UDP
Port: 4789
Source: node subnet
Destination: node subnet
Jeśli masz też:
IPIP → protocol 4
BGP → TCP 179



HELM
curl -O https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3




 
vi my-nginx.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: my-nginx
  name: my-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      run: my-nginx
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - image: nginx
        name: my-nginx
        resources:
          # requests : set minimum required resources when creating pods
          requests:
            # 250m : 0.25 CPU
            cpu: 250m
            memory: 64Mi
          # set maximum resorces
          limits:
            cpu: 500m
            memory: 128Mi
root@ctrl:~# 
vi hpa.yml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-nginx-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    # target Deployment name
    name: my-nginx
  minReplicas: 1
  # maximum number of replicas
  maxReplicas: 4
  metrics:
  - type: Resource
    resource:
      # scale if target CPU utilization is over 20%
      name: cpu
      target:
        type: Utilization
        averageUtilization: 20
root@ctrl:~# 
kubectl apply -f my-nginx.yml -f hpa.yml

NFS

vi nfs-pv.yml
apiVersion: v1
kind: PersistentVolume
metadata:
  # any PV name
  name: nfs-pv
spec:
  capacity:
    # storage size
    storage: 10Gi
  accessModes:
    # Access Modes:
    # - ReadWriteMany (RW from multi nodes)
    # - ReadWriteOnce (RW from a node)
    # - ReadOnlyMany (R from multi nodes)
    - ReadWriteMany
  persistentVolumeReclaimPolicy:
    # retain even if pods terminate
    Retain
  nfs:
    # NFS server definition
    path: /home/nfsshare
    server: 10.4.0.10
    readOnly: false

vi nfs-pvc.yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  # any PVC name
  name: nfs-pvc
spec:
  accessModes:
  - ReadWriteMany
  resources:
     requests:
       storage: 10Gi



vi nginx-nfs.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  # any Deployment name
  name: nginx-nfs
  labels:
    name: nginx-nfs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-nfs
  template:
    metadata:
      labels:
        app: nginx-nfs
    spec:
      containers:
      - name: nginx-nfs
        image: nginx
        ports:
          - name: web
            containerPort: 80
        volumeMounts:
          - name: nfs-share
            # mount point in container
            mountPath: /usr/share/nginx/html
      volumes:
        - name: nfs-share
          persistentVolumeClaim:
            # PVC name you created
            claimName: nfs-pvc





Error from server (BadRequest): container "kube-state-metrics" in pod "prometheus-kube-state-metrics-84df48dc4f-g9g62" is waiting to start: image can't be pulled
docker


# For Helm installations (if using kube-prometheus-stack):
helm upgrade prometheus-stack prometheus-community/kube-prometheus-stack \
  --set kube-state-metrics.image.repository=registry.k8s.io/kube-state-metrics/kube-state-metrics \
  --set kube-state-metrics.image.tag=v2.9.2


helm repo add bitnami https://charts.bitnami.com/bitnami


To jest playbook na węzły dołączane (worker/secondary control-plane też zadziała, ale join musi być odpowiedni).

Masz już działający control-plane i wygenerowany kubeadm join ... (token + hash, opcjonalnie cert key).


Używam containerd jako runtime (najczęściej zalecane).


Kubernetes z repozytorium pkgs.k8s.io (aktualne podejście dla nowszych wersji).
1) Inventory (przykład)
inventory.ini

[k8s_nodes]
worker1 ansible_host=10.4.0.101
worker2 ansible_host=10.0.4.201
[k8s_nodes:vars]
ansible_user=kurs
ansible_become=true


2) Playbook: join-k8s-ubuntu.yml
Wstaw swój kubeadm join ... w zmiennej kubeadm_join_command.

---- name: Install Kubernetes on Ubuntu nodes and join cluster
  hosts: k8s_nodes
  become: true
  vars:
    # UZUPEŁNIJ: wklej pełne polecenie kubeadm join z control-plane
    # przykład: kubeadm join 10.0.0.10:6443 --token abc.def --discovery-token-ca-cert-hash sha256:...
    kubeadm_join_command: ""

    # Wersja K8s (repo z pkgs.k8s.io). Przykładowo v1.35 - dopasuj do klastra.
    k8s_minor: "v1.35"

    # Pakiety: kubelet/kubeadm/kubectl — na workerach kubectl jest opcjonalny, ale często wygodny.
    k8s_packages:
      - kubelet
      - kubeadm
      - kubectl

  pre_tasks:
    - name: Fail if kubeadm_join_command is not set
      ansible.builtin.fail:
        msg: "Ustaw zmienną kubeadm_join_command (wklej pełne polecenie kubeadm join)."
      when: kubeadm_join_command | trim == ""

  tasks:
    - name: Update apt cache
      ansible.builtin.apt:
        update_cache: true
        cache_valid_time: 3600

    - name: Install base dependencies
      ansible.builtin.apt:
        name:
          - ca-certificates
          - curl
          - gpg
          - apt-transport-https
          - software-properties-common
        state: present

    - name: Disable swap (runtime)
      ansible.builtin.command: swapoff -a
      changed_when: false

    - name: Disable swap in /etc/fstab
      ansible.builtin.replace:
        path: /etc/fstab
        regexp: '^([^#].*\s+swap\s+.*)$'
        replace: '# \1'

    - name: Load required kernel modules
      ansible.builtin.copy:
        dest: /etc/modules-load.d/k8s.conf
        content: |
          overlay
          br_netfilter
    - name: Ensure kernel modules are loaded
      ansible.builtin.modprobe:
        name: "{{ item }}"
        state: present
      loop:
        - overlay
        - br_netfilter

    - name: Set sysctl params for Kubernetes networking
      ansible.builtin.copy:
        dest: /etc/sysctl.d/99-kubernetes-cri.conf
        content: |
          net.bridge.bridge-nf-call-iptables  = 1
          net.bridge.bridge-nf-call-ip6tables = 1
          net.ipv4.ip_forward                 = 1
    - name: Apply sysctl params
      ansible.builtin.command: sysctl --system
      changed_when: false

    - name: Install containerd
      ansible.builtin.apt:
        name: containerd
        state: present

    - name: Create containerd config directory
      ansible.builtin.file:
        path: /etc/containerd
        state: directory
        mode: "0755"

    - name: Generate default containerd config (only if missing)
      ansible.builtin.command: containerd config default
      register: containerd_default_config
      changed_when: false

    - name: Write containerd config.toml
      ansible.builtin.copy:
        dest: /etc/containerd/config.toml
        content: "{{ containerd_default_config.stdout }}"
        mode: "0644"

    - name: Enable SystemdCgroup in containerd config
      ansible.builtin.replace:
        path: /etc/containerd/config.toml
        regexp: 'SystemdCgroup = false'
        replace: 'SystemdCgroup = true'

    - name: Restart and enable containerd
      ansible.builtin.systemd:
        name: containerd
        state: restarted
        enabled: true

    - name: Add Kubernetes apt keyring directory
      ansible.builtin.file:
        path: /etc/apt/keyrings
        state: directory
        mode: "0755"

    - name: Add Kubernetes repo key (pkgs.k8s.io)
      ansible.builtin.shell: |
        set -euo pipefail
        curl -fsSL https://pkgs.k8s.io/core:/stable:/{{ k8s_minor }}/deb/Release.key \
          | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg      args:
        executable: /bin/bash
      creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    - name: Add Kubernetes repo
      ansible.builtin.apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/{{ k8s_minor }}/deb/ /"
        state: present
        filename: kubernetes

    - name: Update apt cache after adding Kubernetes repo
      ansible.builtin.apt:
        update_cache: true

    - name: Install Kubernetes packages
      ansible.builtin.apt:
        name: "{{ k8s_packages }}"
        state: present

    - name: Hold Kubernetes packages (prevent unintended upgrades)
      ansible.builtin.dpkg_selections:
        name: "{{ item }}"
        selection: hold
      loop: "{{ k8s_packages }}"

    - name: Enable kubelet
      ansible.builtin.systemd:
        name: kubelet
        enabled: true

    - name: Check if node is already joined (kubelet.conf exists)
      ansible.builtin.stat:
        path: /etc/kubernetes/kubelet.conf
      register: kubelet_conf

    - name: Join the cluster
      ansible.builtin.command: "{{ kubeadm_join_command }}"
      when: not kubelet_conf.stat.exists
      register: join_result
      changed_when: "'This node has joined the cluster' in join_result.stdout or join_result.rc == 0"

    - name: Print join output
      ansible.builtin.debug:
        var: join_result.stdout_lines
      when: join_result is defined
3) Uruchomienie

ansible-playbook -i inventory.ini join-k8s-ubuntu.yml \
  -e 'kubeadm_join_command=kubeadm join 10.0.0.10:6443 --token XXX.YYY --discovery-token-ca-cert-hash sha256:AAAA...'
Jeśli join zawiera spacje/znaki specjalne, czasem wygodniej:

ansible-playbook -i inventory.ini join-k8s-ubuntu.yml \
  -e "kubeadm_join_command=kubeadm join 10.0.0.10:6443 --token XXX.YYY --discovery-token-ca-cert-hash sh


error: error execution phase preflight: 
One or more conditions for hosting a new control plane instance is not satisfied.

unable to add a new control plane instance to a cluster that doesn't have a stable controlPlaneEndpoint address

Please ensure that:
* The cluster has a stable controlPlaneEndpoint address.
* The certificates that must be shared among control plane instances are provided.



kubectl -n kube-system edit cm kubeadm-config

kubectl -n kube-system get cm kubeadm-config -o yaml | sed -n '/ClusterConfiguration:/,$p' | sed -n '1,80p'



  ClusterConfiguration: |
    apiServer: {}
    apiVersion: kubeadm.k8s.io/v1beta4
    caCertificateValidityPeriod: 87600h0m0s
    certificateValidityPeriod: 8760h0m0s
    certificatesDir: /etc/kubernetes/pki
    clusterName: kubernetes
    controllerManager: {}
    controlPlaneEndpoint: "cp1:6443"
    dns: {}
    encryptionAlgorithm: RSA-2048
    etcd:
      local:
        dataDir: /var/lib/etcd
    imageRepository: registry.k8s.io
    kind: ClusterConfiguration
    kubernetesVersion: v1.35.0
    networking:
      dnsDomain: cluster.local
      serviceSubnet: 10.96.0.0/12
    proxy: {}
    scheduler: {}
kind: ConfigMap
metadata:
  creationTimestamp: "2026-02-01T08:47:21Z"
  name: kubeadm-config
  namespace: kube-system
  resourceVersion: "30578"
  uid: 90585441-0bf4-4f8d-b118-6f92bc95ec31

https://www.server-world.info/en/note?os=Ubuntu_24.04&p=kubernetes&f=8

RBAC!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1
